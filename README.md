**Coursera Deep learning tutorial Chinese notes

Course overview

https://mooc.study.163.com/university/deeplearning_ai#/c

These courses are designed for computer professionals who already have a foundation (basic programming knowledge, familiar with Python Python, basic knowledge of machine learning) and want to try to enter the field of artificial intelligence. The introduction shows: "Deep learning is one of the most popular skills in the technology industry. This course will help you master deep learning."

In these five lessons, students will be able to learn the basics of deep learning, learn to build neural networks, and create their own machine learning projects under the guidance of many of the industry's top experts, including Wu Enda himself. **Deep Learning Specialization** Commonly used network structures and tools for deep learning such as convolutional neural networks (**CNN**), recurrent neural networks (**RNN**), long-term and short-term memory (**LSTM**) And knowledge is involved.

There will also be many hands-on projects in the course to help students better apply the deep learning techniques they have learned and solve real-world problems. These projects will cover the fashionable areas of medical, autonomous driving, and natural language processing, as well as music generation and more. **Coursera** has some specific directions and knowledge, but there has been no comprehensive and in-depth deep learning course - the introduction of "Deep Learning Professional" has made up for this vacancy.

The language of the course is **Python**, and the framework used is **Google** open source **TensorFlow**. The most attractive thing is that the course instructor is Wu Enda himself, and both assistants are from the Stanford Computer Department. The time required to complete the course will take approximately 3-4 months depending on the progress of the course. Upon completion of the class, **Coursera** will award them a **Deep Learning Specialization** certificate of completion.

“We will help you master deep learning, understand how to apply deep learning, and start your career in the artificial intelligence industry.” Wu Enda mentioned in the course page.

I have Dr. Huang Haiguang, who has previously written Wu Enda’s machine learning personal notes. At present, I am organizing a team to organize Chinese notes, which will be helped by enthusiastic friends and will be continuously updated. The work of our team is committed to the promotion of **AI** in China, and will not harm the commercial interests of **Coursera** and Wu Enda.

I have a limited level. If there are formulas or algorithm errors, please indicate them in time and send me an email.

**Notes are written based on video and subtitles. There is no technical content, only focus and rigor. **

Huang Haiguang

[我的知乎](https://www.zhihu.com/people/fengdu78/activities)

微信公众号：机器学习初学者 ![gongzhong](images/gongzhong.jpg)
知识星球：黄博的机器学习圈子![xingqiu](images/zhishixingqiu1.jpg)

**注意：github下载太慢的话，关注我的公众号：“机器学习初学者”，回复“github镜像”即可下载本仓库的镜像文件，整个仓库压缩成一个iso。**


[My knowledge](https://www.zhihu.com/people/fengdu78/)

**Main authors**: Huang Haiguang, Lin Xingmu (fourth draft, the first two weeks of the fifth lesson, the first three weeks of the third week), Zhu Yansen: (all the third lesson), He Zhizhen (third lesson Zhou Zhou), Wang Xiang, Hu Yuwen, Yu Xiao, Zheng Hao, Li Huaisong, Zhu Yuepeng, Chen Weihe, Cao Yue, Lu Yixiang, Qiu Muyu, Tang Tianze, Zhang Hao, Chen Zhihao, You Ren, Ze Lin, Shen Weichen, Jia Hongshun, Shi Chao, Chen Zhe, Zhao Yifan, Hu Yuyang, Duan Xi, Yu Chong, Zhang Xinqian

**Participating editors**: Huang Haiguang, Chen Kangkai, Shi Qinglu, Zhong Boyan, Xiang Wei, Yan Fenglong, Liu Cheng, He Zhiwei, Duan Xi, Chen Yao, Lin Jiayong, Wang Xiang, Xie Shichen, Jiang Peng
2018-04-14

**This course video tutorial address: **<https://mooc.study.163.com/university/deeplearning_ai#/c>

** A classmate has provided an offline video download**: link：https://pan.baidu.com/s/1ciq3qHo0lgoD3MLRwfeqnA password：0kim

(This video is downloaded from www.deeplearning.ai. For well-known reasons, it is very difficult for domestic users to watch certain online videos. Therefore, some scholars have produced offline videos together, which are designed to be convenient for domestic users to learn and use. Do not use Commercial use. The video is embedded with Chinese and English subtitles. It is recommended to use **potplayer** to play. The copyright belongs to Wu Enda teacher. If the online video is smooth, please visit the official website.)
[Notes website (suitable for mobile phone reading)] (http://www.ai-start.com)

Wu Enda's machine learning course notes and videos: https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes

**This document is free, please do not use it for commercial purposes and it can be freely distributed. **

**Gifts of roses, hand a fragrance! **

Haiguang2000@qq.com

**Reprint please indicate the source **: https://github.com/fengdu78/deeplearning_ai_books

** Machine learning qq group: 659697409 (we have 9 groups, plus one will not need to add) **



## The first course Neural Networks and Deep Learning

Week 1: Introduction to Deep Learning

1.1 Welcome (Welcome)
1.2 What is a neural network? (What is a Neural Network)

1.3 Supervised Learning with Neural Networks

1.4 Why is neural network popular? (Why is Deep Learning taking off?)

1.5 About this Course (About this Course)

1.6 Course Resources

1.7 Geoffery Hinton interview (Geoffery Hinton interview)

Week 2: Basics of Neural Network Programming

2.1 Binary Classification

2.2 Logistic Regression

2.3 Logistic Regression Cost Function

2.4 Gradient Descent

2.5 Derivatives
2.6 More Derivative Examples

2.7 Computation Graph

2.8 Derivatives with a Computation Graph

2.9 Logistic Regression Gradient Descent

2.10 Gradient Descent on m Examples

2.11 Vectorization

2.12 More Examples of Vectorization

2.13 Vectorizing Logistic Regression

2.14 Vectorizing Logistic Regression's Gradient

2.15 Broadcasting in Python (Broadcasting in Python)

2.16 About the use of Python and numpy vectors (A note on python or numpy vectors)
2.17 Quick Tour of Jupyter/iPython Notebooks

2.18 Explanation of logistic regression cost function

Week 3: Shallow neural networks

3.1 Neural Network Overview (Neural Network Overview)

3.2 Neural Network Representation (Neural Network Representation)

3.3 Calculating a Neural Network's output (Computing a Neural Network's output)

3.4 Vectorizing across multiple examples

3.5 Justification for vectorized implementation

3.6 Activation functions

3.7 Why do I need a nonlinear activation function? (why need a nonlinear activation function?)

3.8 Derivatives of activation functions

3.9 Gradient descent for neural networks

3.10 (optional) intuitive understanding of backpropagation intuition

3.11 Random Initialization (Random+Initialization)

Week 4: Deep Neural Networks

4.1 Deep L-layer neural network

4.2 Forward and backward propagation (Forward and backward propagation)

4.3 Forward Propagation in a Deep Network (Forward Propagation in a Deep Network)

4.4 Checking your matrix dimensions right

4.5 Why use a deep representation? (Why deep representations?)

4.6 Building blocks of deep neural networks

4.7 Parameters vs Hyperparameters

4.8 Deep learning and brain correlation (What does this have to do with the brain?)

Improve deep neural networks: Hyperparameter tuning, Regularization and Optimization (Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization)

Week 1: Practical aspects of Deep Learning

1.1 Training, Verification, Test Set (Train / Dev / Test sets)

1.2 Deviation, variance (Bias /Variance)

1.3 Basic Learning for Machine Learning (Basic Recipe for Machine Learning)

1.4 Regularization

1.5 Why does regularization help prevent overfitting? (Why regularization reduces overfitting?)

1.6 dropout Regularization (Dropout Regularization)

1.7 Understanding dropout (Understanding Dropout)

1.8 Other regularization methods

1.9 Normalizing inputs

1.10 Vanishing / Exploding gradients
1.11 Weight Initialization for Deep Networks Vanishing / Exploding gradients

1.12 Numerical approximation of gradients

1.13 Gradient checking

1.14 Gradient Checking Implementation Notes

Week 2: Optimization algorithms

2.1 Mini-batch gradient descent

2.2 Understanding Mini-batch gradient descent (Understanding Mini-batch gradient descent)

2.3 Exponentially weighted averages

2.4 Understanding Exponentially Weighted Averages

2.5 Bias correction in exponentially weighted averages

2.6 Gradient descent with momentum

2.7 RMSprop - root mean square prop(RMSprop)

2.8 Adam optimization algorithm

2.9 Learning rate decay (Learning rate decay)
2.10 The problem of local optima

The third week of hyperparameter tuning, batch regularization and programming framework (Hyperparameter tuning, Batch Normalization and Programming Frameworks)

3.1 Tuning process

3.2 Using an appropriate scale to pick hyperparameters

3.3 Practice of hyperparameter training: Pandas vs. Caviar (Hyperparameters tuning in practice: Pandas vs. Caviar)

3.4 Normalizing activations in a network

3.5 Fitting Batch Norm into a neural network

3.6 Why does Batch Norm work? (Why does Batch Norm work?)

3.7 Batch Norm (Batch Norm at test time) at the time of testing

3.8 Softmax Regression

3.9 Training a Softmax classifier (Training a softmax classifier)

3.10 Deep learning frameworks

3.11 TensorFlow (TensorFlow)

Structuring Machine Learning Projects

Week 1: Machine Learning Strategies (1) (ML Strategy (1))

1.1 Why is the ML strategy? (Why ML Strategy)

1.2 Orthogonalization

1.3 Single number evaluation metric

1.4 Satisficing and Optimizing metric

1.5 Training set, development set, test set division (Train/dev/test distributions)

1.6 Size of the dev and test sets

1.7 When to change the development set/test set and evaluation indicators (When to change dev/test sets and metrics)

1.8 Why is human performance? (Why human-level performance?)

1.9 Avoidable bias (Avoidable bias)

1.10 Understanding human-level performance

1.11 Surpassing human-level performance

1.12 Improve your model performance (Improving your model performance)
Week 2: Machine Learning Strategies (2) (ML Strategy (2))

2.1 Error analysis (Carrying out error analysis)

2.2 Clearing the incorrectly labeled data (Cleaning up incorrectly labeled data)

2.3 Quickly build your first system and iterate (Build your first system quickly, then iterate)

2.4 Training and testing on different distributions

2.5 Bias and Variance with mismatched data distributions

2.6 Addressing data mismatch

2.7 Transfer Learning

2.8 Multi-task learning

2.9 What is end-to-end deep learning? (What is end-to-end deep learning?)

2.10 Whether to use end-to-end deep learning (Whether to use end-to-end deep learning)

## The fourth course Convolutional Neural Networks

The first week of the Foundation of Convolutional Neural Networks

1.1 Computer vision

1.2 Edge detection example (Edge detection example)

1.3 More edge detection content (More edge detection)

1.4 Padding

1.5 Convolutional Steps (Strided Convolutions)

1.6 Three-dimensional convolution (Convolutions over volumes)

1.7 One layer of a convolutional network

1.8 Simple convolution network example

1.9 Pooling layers

1.10 Convolutional neural network example

1.11 Why use convolution? (Why convolutions?)

Week 2 Deep convolutional models: case studies

2.1 Why do you want to conduct an instance study? (Why look at case studies?)

2.2 Classic networks

2.3 Residual Networks (ResNets)

2.4 Why is the residual network useful? (Why ResNets work?)

2.5 Network in the network and 1×1 convolutions (Network in Network and 1×1 convolutions)

2.6 Google Inception network motivation

2.7 Inception network

2.8 Using open source implementations (Using open-source implementations)

2.9 Transfer Learning

2.10 Data augmentation

2.11 The state of computer vision

Week 3 Target Detection

3.1 Target localization (Object localization)

3.2 Landmark detection

3.3 Target detection (Object detection)

3.4 Convolutional implementation of sliding windows

3.5 Bounding Box predictions

3.6 Intersection over union

3.7 Non-maximum suppression (Non-max suppression)

3.8 Anchor Boxes

3.9 YOLO algorithm (Putting it together: YOLO algorithm)

3.10 Regions (Optional)

Week 4 Special applications: Face recognition & Neural style transfer (Special applications: Face recognition & Neural style transfer)

4.1 What is face recognition? (What is face recognition?)

4.2 One-shot learning (One-shot learning)

4.3 Siamese network

4.4 Triplet loss (Triplet loss)

4.5 Face verification and binary classification

4.6 What is a neural style conversion? (What is neural style transfer?)

4.7 What is a deep convolutional network? (What are deep ConvNets learning?)

4.8 Cost function

4.9 Content cost function

4.10 Style cost function

4.11 1D and 3D generalizations of models

#第五课Sequence Models

First week of the cycle sequence model (Recurrent Neural Networks)
1.1 Why choose a sequence model? (Why Sequence Models?)

1.2 Mathematical Symbols (Notation)

1.3 Recurrent Neural Network Model

1.4 Backpropagation through time

1.5 Different types of different types of RNNs

1.6 Language model and sequence generation (Language model and sequence generation)

1.7 Sampling novel sequences

1.8 Vain gradients disappeared (Vanishing gradients with RNNs)

1.9 GRU unit (Gated Recurrent Unit (GRU))

1.10 long short term memory unit (LSTM (long short term memory) unit)

1.11 Bidirectional RNN (Bidirectional RNN)

1.12 Deep Loop Neural Networks (Deep RNNs)

Week 2 Natural Language Processing and Word Embeddings

2.1 Word Representation

2.2 Using Word Embeddings

2.3 Properties of Word Embeddings

2.4 Embedding Matrix

2.5 Learning Word Embeddings

2.6 Word2Vec

2.7 Negative Sampling

2.8 GloVe Word Vectors

2.9 Sentiment Classification

2.10 Debiasing Word Embeddings

Week 3 Sequence models & Attention mechanism

3.1 Basic Models (Basic Models)

3.2 Picking the most likely sentence

3.3 Cluster Search (Beam Search)

3.4 Improved cluster search (Refinements to Beam Search)

3.5 Error analysis in beam search

3.6 Bleu Score (optional) (Bleu Score (optional))

3.7 Attention Model Intuition

3.8 Attention Model

3.9 Speech recognition

3.10 Trigger Word Detection

3.11 Conclusion and thank you (Conclusion and thank you)

Interview with Artificial Intelligence Master

Wu Enda interviews Geoffery Hinton

Wu Enda interviews Ian Goodfellow
Wu Enda interview Ruslan Salakhutdinov

Wu Enda interviews Yoshua Bengio

Wu Enda Interview Lin Yuanqing

Wu Enda Interview Pieter Abbeel

Wu Enda interviews Andrej Karpathy

annex

Deep Learning Symbol Guide (original course translation)


